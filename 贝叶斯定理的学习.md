# 贝叶斯定理的学习
本文将介绍一种最有用的统计推理方法——贝叶斯规则及其延伸的几个概念。贝叶斯规则以托马斯·贝斯命名，这个规则有很广泛的用途，很容易扩展运用到机器学习当中。
##### ++贝叶斯规则++：
##### ++在贝叶斯学习中使用贝叶斯规则++： 
##### ++选择最优假设++：
##### ++贝叶斯分类++：
### 贝叶斯规则

Charles向我们介绍了本节中关于贝叶斯学习的主要概念。 贝叶斯规则是概率论中的一个重要概念，它让我们可以在对一个问题没有充分把握时做出决策。实际上利用贝叶斯规则可以将先验信息与我们所得到的数据相结合，从而得到可以用来证实我们的猜测的新信息。  最后，贝叶斯规则用下面的公式定义：
```math
P(h|D) = \frac{P(D|h)\,P(h)}{P(D)}
```
其中h代表一种假设，D表示获得的数据。在这个公式中，我们称P(h|D)为后验概率。后验概率可以定义为在考虑相关证据（获得的数据）之后假设条件的概率。我们可以利用先验概率乘以似然函数来计算后验概率。先验概率是我们对假设事件的初始估计概率，用P(h)表示。似然函数是我们从给定假设的概率P(D|h)中获得我们想要的事件的概率。该乘积通过事件概率P(D)标准化，该概率为所有假设事件的可能性之和。所有假设中的后验概率合计为1。通常情况下我们很容易计算出似然概率。  

让我们来看一个例子：  
*“假设我们在一条河里捕鱼，其中60％的鱼是鲶鱼，40％的鱼是条纹鱼。在这个地方我们只能保存5磅以上的鱼。50％的条纹鱼超过5磅，所有鲶鱼都超过5磅。 如果我们保存了一条鱼，这条鱼是条纹鱼的概率有多少？“*

首先我们要对事件的概率做出定义：  
P(S), 表示在不考虑其他因素下，这条鱼是条纹鱼的概率。这个事件的概率为40%。
 
P(C) , 表示在不考虑其他因素下，这条鱼是鲶鱼（或者说不是条纹鱼）的概率，这个概率为60%。
 
P(K|S),表示在这条鱼是条纹鱼的情况下能够保存的概率。我们给出这个事件的概率为50%。 
 
P(K|C) , 表示这条鱼是鲶鱼的情况下能够保存的概率。我们知道这个事件的概率是100%。
 
P(K) , 表示我们能够保存这条鱼的概率。可以使用总概率公式来计算，如下：  

```math
P(K) = P(K|S)P(S) + P(K|C)P(C) = .5 * .4 + .6 * 1 = 0.8(80 \%)
```
这五个概率是我们得到的先验概率（即我们对湖中鱼分布的先验信念）
 
现在我们准备使用贝叶斯定理来解决这个问题。 我们可以找出P(S|K) 
 

```math
P(S|K) = \frac{P(K|S)P(S)}{P(K)} = \frac{0.5*0.4}{0.8} = 0.25 
```
所以我们保存一条鱼，而这条鱼刚好是条纹鱼的概率为25%
 
### 使用贝叶斯规则来进行贝叶斯定理的学习：
 
我们可以将贝叶斯规则应用到我们训练的数据中，从而出判断出最优假设。通过找出所有给定假设事件中中概率最大的事件来做到这一点。
用数学符号来表示，如下：  

```math
h_{MAP} = argmax_hP(h|D)\forall h \in H

```
h的MAP下标表示最大后验概率，最大后验给出了所有的先验。

我们计算argmax时和前面的数据并不完全相关。也就是说，我们不用关心分母中的P(D)项会影响所有计算。这对我们有很大帮助，因为我们经常会发现找出P(D)是相当困难的一件事。在某些情况下如果我们假设所有P(h)都是等价的，我们可以使用最大似然计算出P(h|D)。最大似然通常用ML来表示，算法为：

```math
h_{ML} = argmax_hP(D|h)\forall h \in H
```

### 选择最优假设：
 我们的最终目标是选择最优假设来预测我们的数据。 然而有时候预测的结果之间差异可能很小（例如：假设h1 = 0.7,h2 = 0.71，我们应该选择h1或h2)。 奥卡姆剃刀定律中指出，在竞争的假设中，应该选择具有最小位数的那个。我们如何去找到位数最短的假设？ 以下是如何找到最小描述长度的方法： 

```math
h_{MDL} = argmin_{h \in H} L_{C1}(h) + L_{C2}(D|h) 
```
这里的 LC(x) 是编码C下的描述长度x
 
例如：  
H=决策树 和 D =练习数据的标签
 
LC1(h)是描述树h的位数  
LC2(D|h)是在给定h的情况下描述D的位数 
 
从这里，我们可以看到hMDL折衷树的训练误差大小
 
### 贝叶斯分类： 
 
到目前为止，我们发现了给定数据集D的最可能的假设hMAP。现在的问题是给出一个新的x，那么最可能的分类是什么?不幸的是，hMAP(x)并不总是最可能的分类。

我们需要提出贝叶斯最优分类器，它可以定义为：

```math
argmax_{vj \in V} \sum_{h_j \in H}P(v_j|h_i)P(h_i|D) 
```
其中vj来自由H分配的可能分类的集合。
 
例如： 
 
假设我们给定：  

        P(h1|D) = 0.4        P(h2|D) = 0.3          P(h3|D) = 0.3  
    h1(x) = +            h2(x) = −              h3(x) = − 
 
利用给我们的信息找出x最可能的分类  
        V = {−,+}   
P(− |h1) = 0, P(+ |h1) = 1  
P(− |h2) = 1, P(+ |h2) = 0  
P(− |h3) = 1, P(+ |h3) = 0  
 
即  
 
```math
\sum_{h_i \in H}P(+ |h_i)P(h_i|D) = 0.4  |
\sum_{h_i \in H}P(- |h_i)P(h_i|D) = 0.6
```
因此我们将得到最优分类器
 
在下一课中，你将找到改进此分类过程的方法！
